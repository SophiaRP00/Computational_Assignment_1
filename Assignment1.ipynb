{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02467 - Computational Social Science: Assignment 1\n",
    "#### Group 15: Adam Bøttcher Haupt-Hansen, s224202 & Edvin Smajlovic, s224204 & Sophia Reiffenstein Petersen, s224222 \n",
    "\n",
    "Everyone in the group has contributed equally to this project, as we have worked together weekly on the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to GitLab Repository\n",
    "GitRep = \"https://gitlab.gbar.dtu.dk/s224222/computational_assignment1\"\n",
    "Github = \"https://github.com/SophiaRP00/Computational_Assignment_1/tree/main\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import regex as re\n",
    "import math\n",
    "import os\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I - WEBSCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claudia Wagner', 'Jonas L Juul', ' Jon Kleinberg', 'Chloe Ahn', ' Xinyi Wang']\n"
     ]
    }
   ],
   "source": [
    "# Get all the names into a table\n",
    "names = []\n",
    "for i in soup.find_all(\"i\"):\n",
    "    names.append(i.text)\n",
    "\n",
    "names = \",\".join(names)\n",
    "\n",
    "\n",
    "# if \"chair\" in names: Remove the \"chair\"\n",
    "\n",
    "names = names.replace(\"Chair: \", \"\")\n",
    "names = names.split(\",\")\n",
    "map(str.strip, names)\n",
    "print(names[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron clauset', 'aaron j. schwartz', 'aaron schein', 'aaron smith', 'abbas haidar']\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates, strip and lower case\n",
    "for i in range(len(names)):\n",
    "    names[i] = names[i].lower()\n",
    "    names[i] = names[i].strip()\n",
    "names = list(set(names))\n",
    "# and sort alphabetically\n",
    "\n",
    "names.sort()\n",
    "print(names[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alessandro flammini', 'anne c. kroon', 'diogo pacheco', 'duncan j. watts', 'fabio carrella']\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates where the names are very similar using sequence matcher\n",
    "\n",
    "duplicate = []\n",
    "for i in range(len(names)):\n",
    "    for j in range(i+1, len(names)):\n",
    "        if difflib.SequenceMatcher(None, names[i], names[j]).ratio() > 0.95:\n",
    "            duplicate.append(names[j].lower().strip())\n",
    "\n",
    "print(duplicate[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one of the duplicates from the list\n",
    "names = [x for x in names if x not in duplicate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471\n"
     ]
    }
   ],
   "source": [
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 1471 authors after having cleaned the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a txt file with the names\n",
    "\n",
    "with open(\"Data/names.txt\", \"w\") as f:\n",
    "    for name in names:\n",
    "        f.write(name + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE PROCESS:\n",
    "\n",
    "In order to retrieve as many names as possible, we noticed that all the authors were written in italics in the Plenary programs. Additionally, authors were written after the word \"Keynote\" in the overview which we also took into consideration. \n",
    "\n",
    "To clean the data, we lowercased, striped and removed obvious duplicates. To take human error into account, we used a sequence matcher from the difflib library where we compared each name and if they were 95% or more similar, we removed them. We checked each of them by printing the two similar names, and thereby, trying to ensure we didn't remove any that could potentially be be two seperate people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II - READY MADE VS. CUSTOM MADE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION:\n",
    "\n",
    "What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words).\n",
    "\n",
    "#### ANSWER: <br>\n",
    "The pros of custom-made data as seen in Centola's experiments is that the experiment was designed in a controlled environment, reducing confounding variables and making it easuer to establish relationsships between people in Centola's experiment. However, there are quite a lot of cons when using custom-made data such as limited generalisability and smaller sample sizes. The largest issue is being nonreactive which can explain how certain people might perform better, since they are being watched. \n",
    "\n",
    "When it comes to Nicolaide's study, we see that it provides a better insight into socital interactions as it is performed on \"real-life data. However, we see that Ready-made data is generally sensitive as well as dirty and lacks reproduceability. This is also the case when it comes to Nicolaide's study where the data might reflect real-life networks compared to custom-made but it comes at the cost of the privacy of the participants. \n",
    "\n",
    "Reference: Chapter 2.3: Salgani, Matthew J. - Bit by bit (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: <br>\n",
    "How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)\n",
    "\n",
    "#### ANSWER <br>\n",
    "Using custom-made data is good when it comes to reproducability, but it might lead to wrong conclusions about societal networks. Ready-made data is more likely to lead to a more realistic interpretation about real-life situation, but as it is hard to reproduce, it might in that sense be difficult to argue for generability. Additionally, there might be unobserved confounders or homophily like mentioned in the video where people are friends with similar people or holidays or weather affects our actions. These will generally affect our data analysis and might lead to false conclusions.\n",
    "\n",
    "Therefore, it is important to mix both custom-made and ready-made data which is often seen in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART III - GATHERING RESEARCH ARTICLES USING OPENALEX API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since actually building the csv file of the 2024 authors is not a part of the assignment, we can simply load in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(\"Data/authors2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1047"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created to filter what papers we actually want to look at\n",
    "fields_to_ids = {'Sociology': 'https://openalex.org/C144024400',\n",
    "                 'Psychology': 'https://openalex.org/C15744967',\n",
    "                 'Economics': 'https://openalex.org/C162324750',\n",
    "                 'Political Science': 'https://openalex.org/C17744445',\n",
    "                 'Mathematics': 'https://openalex.org/C33923547}',\n",
    "                 'Physics':'https://openalex.org/C121332964',\n",
    "                 'Computer Science': 'https://openalex.org/C41008148'\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Showing the basic structure of the API request\n",
    "base_url = 'https://api.openalex.org/' \n",
    "resource = 'works'\n",
    "filterstring = [\"cited_by_count:>10\",\n",
    "                 \"authors_count:<10\", \n",
    "                 f\"concepts.id:{fields_to_ids['Sociology']}|{fields_to_ids['Psychology']}|{fields_to_ids['Economics']}|{fields_to_ids['Political Science']}\", \n",
    "                 f\"concepts.id:{fields_to_ids['Mathematics']}|{fields_to_ids['Physics']}|{fields_to_ids['Computer Science']}\"]\n",
    "filterstring = \",\".join(filterstring)\n",
    "\n",
    "request = {\n",
    "    \"filter\": filterstring,\n",
    "    \"cursor\": '*',\n",
    "}\n",
    "if (os.environ.get('mail') is not None): ## get email from environment variable, so we don't expose it in the code\n",
    "    request['mailto'] = os.environ.get('mail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = requests.get(base_url + resource, params=request)\n",
    "data = rq.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We can get data from the api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring we keep the authors only within limits\n",
    "authors = authors[(authors['works_count'] > 5) & (authors['works_count'] < 5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = authors['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://openalex.org/A5097398930',\n",
       "       'https://openalex.org/A5082554858',\n",
       "       'https://openalex.org/A5067206551',\n",
       "       'https://openalex.org/A5014394213',\n",
       "       'https://openalex.org/A5008439962'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_ids[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly define some helper functions for parsing the papers and handling individual requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paper(paper):\n",
    "    paper_id = paper['id']\n",
    "    pub_year = paper['publication_year']\n",
    "    cited_by = paper['cited_by_count']\n",
    "    author_ids = [author['author']['id'] for author in paper['authorships']]\n",
    "    title = paper['title']\n",
    "    abstract_inverted_index = paper['abstract_inverted_index']\n",
    "    return [paper_id, pub_year, cited_by, author_ids, title, abstract_inverted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_papers(index):\n",
    "    query = author_ids[index: index+10]   \n",
    "    author_string = \"authorships.author.id:\" + \"|\".join(query)\n",
    "    cursor = '*'\n",
    "    request = {\n",
    "        \"filter\": filterstring + \",\" + author_string,\n",
    "        \"cursor\": cursor,\n",
    "        'per_page': 100\n",
    "    }\n",
    "    if (os.environ.get('mail') is not None): ## Same deal as before. I dont particularly want to expose my email in a public repo\n",
    "        request['mailto'] = os.environ.get('mail')\n",
    "    parsed_results = []\n",
    "    while cursor:\n",
    "        rq = requests.get(base_url + resource, params=request)\n",
    "        if rq.status_code != 200:\n",
    "            raise Exception(rq.status_code)\n",
    "        try:\n",
    "            meta = rq.json()['meta']\n",
    "            cursor = meta['next_cursor']\n",
    "        except:\n",
    "            cursor = False\n",
    "            break\n",
    "        request['cursor'] = cursor\n",
    "        sleep(0.6) ## Arguably a long sleep, but better safe than sorry\n",
    "        results = rq.json()['results']\n",
    "        for paper in results:\n",
    "            parsed_results.append(parse_paper(paper))\n",
    "    return parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:37<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "## This is slightly inconsistent. Seems like the API just sometimes fails\n",
    "final_results = Parallel(n_jobs=8, prefer='threads')(delayed(get_author_papers)(i) for i in tqdm(range(0, len(author_ids), 10))) ## Using joblib to parallelize the requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having gathered the results, we can then simply save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscpapers = []\n",
    "for res in final_results:\n",
    "    iscpapers.extend(res)\n",
    "\n",
    "\n",
    "iscpapers = pd.DataFrame(iscpapers, columns=['paper_id', 'pub_year', 'cited_by', 'author_ids', 'title', 'abstract_inverted_index'])\n",
    "\n",
    "iscpapers_2 = iscpapers[['paper_id', 'pub_year', 'cited_by', 'author_ids']]\n",
    "isc_abstracts = iscpapers[['paper_id', 'title', 'abstract_inverted_index']]\n",
    "iscpapers_2.to_csv('Data/iscpapers.csv', index=False)\n",
    "isc_abstracts.to_csv('Data/isc_abstracts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11595"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iscpapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_authors = iscpapers['author_ids'].explode().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15666"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(paper_authors['author_ids'].to_numpy()))) ## Unholy way to get the number of unique authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?\n",
    "#### Answer\n",
    "There are 11595 papers in our dataframe. These papers have been authored by a total of 15666 authors\n",
    "#### Question:\n",
    "Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?\n",
    "#### Answer:\n",
    "We used several strategies for speed. We used filters in our both our own author list, and in our api requests to make sure that every result returned was actually relevant. Additionally we grouped 10 authors per request as most authors only have a few papers, so getting more results back from each request allowed for higher speeds. Finanally we used multithreading, to enable multiple simultaneous calls. This made the code run quite fast, in fact, it includes a sleep(0.6) because it kept running too fast for the api.\n",
    "#### Question:\n",
    "Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? \n",
    "#### Answer:\n",
    "Filtering criteria help guide the collection of data. That said, in a relatively new field such as Computational Social Science, limiting ourselves to only papers with \\>10 citations, can potentially, limit our understanding of more recent changes. As this is data from a conference in 2024, this risks eliminating new researchers, or papers published recently. Additionally the topic filter has problems, due to how OpenAlex structures its topics. This potentially results in actual computational social science getting drowned out by papers in other genres, that happed to be tagged with topics like \"mathematics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART IV - THE NETWORK OF COMPUTATIONAL SCIENTISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_combined = pd.read_csv('Data/IC2S2_combined_papers.csv')\n",
    "authors_combined = pd.read_csv('Data/IC2S2_combined_authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Nodes:17729\n",
      "Total number of Edges:71979\n"
     ]
    }
   ],
   "source": [
    "#We construct our graph and all the weighted edges between authors\n",
    "graph = nx.Graph()\n",
    "for paper_authors in papers_combined['author_ids']:\n",
    "    authors = re.sub(r\"[\\[\\]'\\s]\", \"\", paper_authors).split(',')\n",
    "    for i,author1 in enumerate(authors):\n",
    "        for author2 in authors[i:]:\n",
    "            if author1 != author2:\n",
    "                if graph.has_edge(author1,author2):\n",
    "                    graph[author1][author2]['weight'] += 1\n",
    "                else:\n",
    "                    graph.add_edge(author1,author2,weight=1)\n",
    "print(\"Total number of Nodes:\" + str(graph.number_of_nodes()))\n",
    "print(\"Total number of Edges:\" + str(graph.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find the attributes of the authors\n",
    "citationcount = {}\n",
    "publication_years = {}\n",
    "\n",
    "for i,paper_authors in enumerate(papers_combined['author_ids']):\n",
    "    authors = re.sub(r\"[\\[\\]'\\s]\", \"\", paper_authors).split(',')\n",
    "    for author in authors:\n",
    "        if author not in citationcount:\n",
    "            citationcount[author] = 0\n",
    "        citationcount[author] += 1\n",
    "\n",
    "        if author not in publication_years:\n",
    "            publication_years[author] = papers_combined['publication_year'][i]\n",
    "        else:\n",
    "            if publication_years[author] > papers_combined['publication_year'][i]:\n",
    "               publication_years[author] = papers_combined['publication_year'][i]\n",
    "\n",
    "def get_author_info(author_id):\n",
    "    author_info = authors_combined[authors_combined['id'] == author_id]\n",
    "    display_name = author_info['display_name'].values[0]\n",
    "    country_code = author_info['country_code'].values[0]\n",
    "    return display_name, country_code, citationcount[author_id], publication_years[author_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://openalex.org/A5014647140', {'country_code': 'US', 'display_name': 'Aaron Clauset', 'citation_count': 36, 'first_publication_year': 2004})\n",
      "('https://openalex.org/A5082953212', {'country_code': 'US', 'display_name': 'Cosma Rohilla Shalizi', 'citation_count': 23, 'first_publication_year': 1999})\n",
      "('https://openalex.org/A5067142016', {'country_code': 'US', 'display_name': 'M. E. J. Newman', 'citation_count': 55, 'first_publication_year': 1994})\n",
      "('https://openalex.org/A5008033989', {'country_code': 'US', 'display_name': 'Cristopher Moore', 'citation_count': 31, 'first_publication_year': 1996})\n",
      "('https://openalex.org/A5007285525', {'country_code': 'US', 'display_name': 'Erzsébet Ravasz Regan', 'citation_count': 8, 'first_publication_year': 2000})\n"
     ]
    }
   ],
   "source": [
    "#We add the node attributes to the graph      Runtime: ~20s\n",
    "for author in graph.nodes():\n",
    "    display_name, countrycode, citation_count, first_publication_year = get_author_info(author)\n",
    "    if type(countrycode) != str and math.isnan(countrycode):\n",
    "        countrycode = 'NaN'\n",
    "\n",
    "    nx.set_node_attributes(graph, {author: countrycode}, 'country_code')\n",
    "    nx.set_node_attributes(graph, {author: display_name}, 'display_name')\n",
    "    nx.set_node_attributes(graph, {author: citation_count}, 'citation_count')\n",
    "    nx.set_node_attributes(graph, {author: int(first_publication_year)}, 'first_publication_year')\n",
    "\n",
    "#We print the first 5 nodes of the graph and save it as JSON\n",
    "for node in list(graph.nodes(data=True))[:5]:\n",
    "   print(node)\n",
    "\n",
    "import json\n",
    "with open('Data/graph.json', 'w') as f:\n",
    "    json.dump(nx.node_link_data(graph), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Preliminary Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of authors: 17729\n",
      "Total number of unique collaborations: 71979\n",
      "\n",
      "Network Density: 0.00045802778209354516\n",
      "\n",
      "Number of connected components: 107\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of authors: \" + str(graph.number_of_nodes()))\n",
    "print(\"Total number of unique collaborations: \" + str(graph.number_of_edges()))\n",
    "print()\n",
    "print(\"Network Density: \" + str(nx.density(graph)))\n",
    "print()\n",
    "print(\"Number of connected components: \" + str(nx.number_connected_components(graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION:<br>\n",
    "Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
    "If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset.\n",
    "How many isolated nodes are there in your network? An isolated node is defined as a node with no connections to any other node in the network.\n",
    "Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)\n",
    "\n",
    "#### ANSWER <br>\n",
    "We see above that we have captured rougly 18 thousand authors with 72 thousand collaborations between them <br>\n",
    "The network is sparse with a density of  around 0.000458. This makes sense as the maximum number of links is n*(n-1)/2 or 157149856 in this case, and we have nowhere near that many collaborations. <br>\n",
    "We also see that the graph has 107 connected components, meaning the network is disconnected. <br>\n",
    "Finally we know there are no isolated authors, as we constructed the graph (and hence all the nodes) entirely from edges <br>\n",
    "<br>\n",
    "These findings make sense, as most authors will not collaborate with anywhere near 18 thousand other authors, so the network density will be low. The graph not being connected also makes sense, as different fields and locations might prevent authors from co-authoring with others who are either far-away or working on a different topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average degree:8.11991652095437\n",
      "Median degree:6.0\n",
      "Mode degree:4\n",
      "Minimum degree:1\n",
      "Maximum degree:362\n",
      "\n",
      "Average strength:15.034124880139883\n",
      "Median strength:8.0\n",
      "Mode strength:4\n",
      "Minimum strength:1\n",
      "Maximum strength:605\n"
     ]
    }
   ],
   "source": [
    "# Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree)\n",
    "import numpy as np\n",
    "degrees = [degree for node,degree in graph.degree()]\n",
    "strengths = [strength for node,strength in graph.degree(weight='weight')]\n",
    "\n",
    "print(\"Average degree:\" + str(np.mean(degrees)))\n",
    "print(\"Median degree:\" + str(np.median(degrees)))\n",
    "print(\"Mode degree:\" + str(max(set(degrees), key=degrees.count)))\n",
    "print(\"Minimum degree:\" + str(min(degrees)))\n",
    "print(\"Maximum degree:\" + str(max(degrees)))\n",
    "print()\n",
    "print(\"Average strength:\" + str(np.mean(strengths)))\n",
    "print(\"Median strength:\" + str(np.median(strengths)))\n",
    "print(\"Mode strength:\" + str(max(set(strengths), key=strengths.count)))\n",
    "print(\"Minimum strength:\" + str(min(strengths)))\n",
    "print(\"Maximum strength:\" + str(max(strengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: <br>\n",
    "What do these metrics tell us about the network? (answer in max 150 words)\n",
    "\n",
    "#### ANSWER <br>\n",
    "These metrics tell us that most authors in our graph have \"only\" collaborated with 4 other authors, while some few collaborate with up to 362 different authors, dragging the average up. We also see that the mode strength and mode degree are the same, implying that most authors in our graph who've only collaborated with 4 other authors, have only collaborated with them once (most authors have low strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 authors with the highest degree:\n",
      "Yan Wang with degree 362    OpenAlex ID: https://openalex.org/A5100322712\n",
      "Yi Yang with degree 306    OpenAlex ID: https://openalex.org/A5005421447\n",
      "Simon A. Levin with degree 279    OpenAlex ID: https://openalex.org/A5077712228\n",
      "Alex Pentland with degree 261    OpenAlex ID: https://openalex.org/A5007176508\n",
      "Robert West with degree 255    OpenAlex ID: https://openalex.org/A5059645286\n"
     ]
    }
   ],
   "source": [
    "#We find the 5 authors with the highest degree\n",
    "degree_dict = dict(graph.degree())\n",
    "sorted_degree = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 5 authors with the highest degree:\")\n",
    "for i in range(5):\n",
    "    display_name, country_code, citation_count, first_publication_year = get_author_info(sorted_degree[i][0])\n",
    "    print(display_name + \" with degree \" + str(sorted_degree[i][1]) + \"    OpenAlex ID: \" + str(sorted_degree[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: <br>\n",
    "Identify the top 5 authors by degree. What role do these node play in the network?\n",
    "Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)\n",
    "\n",
    "#### ANSWER <br>\n",
    "Looking at the top 5 authors, they are not very aligned with the themes of Computational Social Science, with the exception of Robert West. The others mainly have papers concerning math, biology, and such. This is could be due to many reasons, but we think it's likely because of these fields being bigger and the topics might also be more prone to collaboration than Social Science"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
